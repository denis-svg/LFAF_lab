#+title: Lab2
#+PROPERTY: header-args:python   :session :exports both :eval no-export
* Implementation of formal languages
- Course :: Formal Languages & Finite Automata
- Author :: Balan Artiom

* Theory
** Lexical analysis
In English, words are composed of letters and words have meanings,
but the meaning isn't derived from the letters.
In fact, we don't even think of the letters when we read sentences,
we read the words as a whole and only care about their meaning.
That's kind of an analogy to the fact that parsers don't really care about the characters,
they care about syntactic units, called lexemes.

A lexeme is a string of characters that has a meaning.
Lexemes often correspond to terminals in a grammar (e.g. identifier, number, operator).

It's useful to store the location and length of each lexeme.
The data structure unit used to store lexemes together with information about them is called a token.
** Ambiguous grammar
Lexers for real programming languages often can't be constructed from the grammar alone,
since there are rules not captured in the grammar:
1) The off-side rule (indentation-sensitive blocks) can't be described by context-free grammars
2) Grammar rules can be ambiguous

When grammar rules are ambiguous, a string can be matched by multiple rules.
To counter this, for example ANTLR has disambiguating rules for tokenization ([[https://github.com/antlr/antlr4/blob/49b69bb31aa34654676a864b229a369680122470/doc/wildcard.md#nongreedy-lexer-subrules][docs]]):
+ Greedy and non-greedy regex lexer rules
+ Match the first rule occurring in the grammar

Ambiguous constructs should be used sparingly and in a strictly controlled fashion;
otherwise, there can be no guarantee as to what language is recognized by a parser [cite:@aho2007compilers].

One quirk that proves disambiguization is complicated is the way ANTLR handles non-greedy rules (see rule 4 [[https://github.com/antlr/antlr4/blob/49b69bb31aa34654676a864b229a369680122470/doc/wildcard.md#nongreedy-lexer-subrules][in this section]]).

A token has a name and an optional value, which can be of any type (including =dict=).
Token names can correspond to nonterminals in the grammar,
but can also be groupings of terminals (e.g. "operator").

Usually, whitespace doesn't make it past the lexer, but is still necessary to separate lexemes.
For example, =elsex= is an *idendtifier*, but =else x= is the keyword *else* and the *identifier* /x/.
* Objectives
- [X] Implement a lexer and show how it works.
* Results
I wrote a lexer for python-like syntax, hence, all the example strings are valid python code.

#+begin_src python :exports none
import sys, os
sys.path.append(os.path.join(os.path.dirname(sys.argv[0]), '..', 'src'))
from lexer import *

def tabulate_tokens(s):
    ls = get_tokens(inp)
    from tabulate import tabulate
    return tabulate([("={}=".format(t.type), "={}=".format(t.value) if t.value else '') for t in ls], tablefmt="orgtbl", headers=["Token name", "Token value"])
#+end_src

#+RESULTS:

Let's parse a simple variable assignment:
#+name: input
#+begin_src python
a_1 = 12 * 3 + 2
#+end_src

#+RESULTS: input

Each token is represented by two things: a name and an optional value.
In this example, notice that the token for the variable =a= is of type =ID=,
which stands for "identifier", and the token value is the name of the variable.

Similarly, numbers are represented by =NUMBER= tokens, with their value as the token value.
#+begin_src python :var inp=(get-val-of-named-src-block "input") :exports results :results drawer
tabulate_tokens(inp)
#+end_src

#+RESULTS:
:results:
| Token name         | Token value |
|--------------------+-------------|
| =TokenType.ID=     | =a_1=       |
| ===                |             |
| =TokenType.NUMBER= | =12=        |
| =*=                |             |
| =TokenType.NUMBER= | =3=         |
| =+=                |             |
| =TokenType.NUMBER= | =2=         |
| =TokenType.EOF=    |             |
:end:

Now let's see how a lexer recognizes indentation:
#+name: inp2
#+begin_src python
def t(arg):
    print(arg)
#+end_src

#+RESULTS: inp2

#+begin_src python :var inp=(get-val-of-named-src-block "inp2") :exports results :results drawer
tabulate_tokens(inp)
#+end_src

#+RESULTS:
:results:
| Token name         | Token value |
|--------------------+-------------|
| =TokenType.DEFN=   |             |
| =TokenType.ID=     | =t=         |
| =(=                |             |
| =TokenType.ID=     | =arg=       |
| =)=                |             |
| =:=                |             |
| =TokenType.INDENT= |             |
| =TokenType.ID=     | =print=     |
| =(=                |             |
| =TokenType.ID=     | =arg=       |
| =)=                |             |
| =TokenType.DEDENT= |             |
| =TokenType.EOF=    |             |
:end:

Did you catch that?
The lexer generated two additional "invisible" tokens
to let the parser know about the indented block: =INDENT= and =DEDENT=.

You could visualize the token placement like this:
#+begin_example
1. def t(arg):
     v INDENT
2.    print(arg)
3.
  ^ DEDENT
#+end_example

Let's see a more complicated example:
#+name: inp3
#+begin_src python :eval no
if a:
    if b:
        foo()
bar()
#+end_src

#+begin_src python :var inp=(get-val-of-named-src-block "inp3") :exports results :results drawer
tabulate_tokens(inp)
#+end_src

#+RESULTS:
:results:
| Token name         | Token value |
|--------------------+-------------|
| =TokenType.IF=     |             |
| =TokenType.ID=     | =a=         |
| =:=                |             |
| =TokenType.INDENT= |             |
| =TokenType.IF=     |             |
| =TokenType.ID=     | =b=         |
| =:=                |             |
| =TokenType.INDENT= |             |
| =TokenType.ID=     | =foo=       |
| =(=                |             |
| =)=                |             |
| =TokenType.DEDENT= |             |
| =TokenType.DEDENT= |             |
| =TokenType.ID=     | =bar=       |
| =(=                |             |
| =)=                |             |
| =TokenType.EOF=    |             |
:end:

Let's visualize this too:
#+begin_example
1. if a:
     v INDENT
2.    if b:
          v INDENT
3.         foo()
  ^ 2 x DEDENT
4. bar()
#+end_example

Notice how two =DEDENT= tokens were generated before =bar()=,
because we "closed" two indented blocks.

The lexer recognizes comments too and ignores them:
#+name: inp4
#+begin_src python :eval no
 # this line has a bad indent
def t(arg):
    print(arg)  # this comment is inline
#+end_src

#+begin_src python :var inp=(get-val-of-named-src-block "inp4") :exports results :results drawer
tabulate_tokens(inp)
#+end_src

#+RESULTS:
:results:
| Token name         | Token value |
|--------------------+-------------|
| =TokenType.DEFN=   |             |
| =TokenType.ID=     | =t=         |
| =(=                |             |
| =TokenType.ID=     | =arg=       |
| =)=                |             |
| =:=                |             |
| =TokenType.INDENT= |             |
| =TokenType.ID=     | =print=     |
| =(=                |             |
| =TokenType.ID=     | =arg=       |
| =)=                |             |
| =TokenType.DEDENT= |             |
| =TokenType.EOF=    |             |
:end:

Notice that the first line has a bad indent (first line can't be indented in python),
but since it's a comment, we can ignore this issue (one more edge-case to consider).

There's one type of indentation error that can be recognized by the lexer (and 3 others that can only be recognized by the parser),
and that's the "inconsistent dedent":
#+name: inp5
#+begin_src python :eval no
def foo(a):
    if a == 1:
        return 1
   return 0
#+end_src

The lexer simply raises an exception for this example.
* Implementation
Indentation handling is implemented as described in the [[https://docs.python.org/3/reference/lexical_analysis.html#indentation][python docs]].

The entire "lexer" is a single function =get_tokens(s) -> ls=
that takes a string to be tokenized, and returns a list of all the tokens.

Initially I tried wrapping the tokenizer inside a class, but it didn't make sense
and only made things more obscure and complicated.
I don't see why you would need to maintain the state of a lexer by reading tokens one by one,
when you could instead get all the tokens at once.
And if you don't need a state, there's no need for an object.

The =get_tokens= function reads characters using either =getch()=  or =peek()=,
depending on whether it wants to also consume the character.

The entire function is a loop that tokenizes the entire string,
until there's no more characters left, after which it generates the last token, =EOF=.
